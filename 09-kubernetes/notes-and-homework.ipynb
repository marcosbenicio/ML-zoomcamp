{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Bulding the image from homework 05**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to the `ML-zoomcamp/05Deploy/Homework` repo and build the docker image from the Dockerfile of the homework 05.\n",
    "\n",
    "`Terminal`\n",
    "\n",
    "----\n",
    "\n",
    "```bash\n",
    "marcos@marcos:~$ docker build -t zoomcamp-model:hw10 .\n",
    "```\n",
    "\n",
    "----\n",
    "\n",
    "the content of the Dockerfile is:\n",
    "\n",
    "`Dockerfile`\n",
    "\n",
    "----\n",
    "\n",
    "```docker \n",
    "# Start with the existing image as a base\n",
    "FROM svizor/zoomcamp-model:3.10.12-slim\n",
    "\n",
    "# Set environment variables\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "\n",
    "# Install pipenv\n",
    "RUN pip --no-cache-dir install pipenv\n",
    "\n",
    "# Set the working directory inside the container\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy the Flask script, Pipenv files into the container\n",
    "COPY [\"question6.py\", \"Pipfile\", \"Pipfile.lock\", \"./\"]\n",
    "\n",
    "# Install Python dependencies and clean cache\n",
    "RUN pipenv install --deploy --system && \\\n",
    "rm -rf /root/.cache\n",
    "\n",
    "# Port the app runs on\n",
    "EXPOSE 9696\n",
    "\n",
    "# Run Gunicorn\n",
    "ENTRYPOINT [\"gunicorn\", \"--bind\", \"0.0.0.0:9696\", \"question6:app\"]\n",
    "```\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Question 1**\n",
    "\n",
    "Run it to test that it's working locally:\n",
    "\n",
    "```bash\n",
    "docker run -it --rm -p 9696:9696 zoomcamp-model:hw10\n",
    "```\n",
    "\n",
    "And in another terminal, execute `q6_test.py` file:\n",
    "\n",
    "```bash\n",
    "python q6_test.py\n",
    "```\n",
    "\n",
    "You should see this:\n",
    "\n",
    "```python\n",
    "{'get_credit': True, 'get_credit_probability': <value>}\n",
    "```\n",
    "\n",
    "Here `<value>` is the probability of getting a credit card. You need to choose the right one.\n",
    "\n",
    "* 0.3269\n",
    "* 0.5269\n",
    "* **0.7269**\n",
    "* 0.9269"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Credit probability': 0.726936946355423}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:9696/predict\"\n",
    "client = {\"job\": \"retired\", \"duration\": 445, \"poutcome\": \"success\"}\n",
    "response = requests.post(url, json=client).json()\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Installing `kubectl` and `kind`**\n",
    "\n",
    "You need to install:\n",
    "\n",
    "* `kubectl` - https://kubernetes.io/docs/tasks/tools/ (you might already have it - check before installing)\n",
    "* `kind` - https://kind.sigs.k8s.io/docs/user/quick-start/\n",
    "\n",
    "\n",
    "## **Question 2**\n",
    "\n",
    "What's the version of `kind` that you have? \n",
    "\n",
    "Use `kind --version` to find out.\n",
    "\n",
    "`Terminal`\n",
    "\n",
    "----\n",
    "```bash\n",
    "marcos@marcos:~$ kind --version\n",
    "kind version 0.20.0\n",
    "```\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kubernetes\n",
    "\n",
    "Docker is used for running containers, which are packages of code and dependencies ensuring applications run efficiently across different environments (computers, cloud, virtual machines). However, for  managing these containers, especially at large scale, is necessary the use of Kubernetes tool.\n",
    "\n",
    "Kubernetes is an open-source platform that manage container applications and services. It enables declarative configuration and automation, making it easier to handle applications in various environments like physical servers, virtual machines, or cloud systems. \n",
    "\n",
    "While Docker focuses on packaging and running individual containers, Kubernetes provides a comprehensive framework for orchestrating those containers in a clustered environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main unit of abstraction in Kubernetes is a **pod**. A pod can contain one or more containers, typically Docker containers. The pods live within a **node**, which can be either a virtual machine or a physical computer that runs the Kubernetes processes. The nodes are grouped into **Kubernetes clusters**.\n",
    "\n",
    "\n",
    "<center><img src = \"figures/kubernets-request.png\" width=\"600\" height=\"300\"/></center>\n",
    "\n",
    "\n",
    "The pods within a deployment (a group of pods) usually contain instances of the same container image. A client can make a request, where the entry point is the **Ingress**, which communicates with the external gateway service. The **gateway service** (often referred to as an ingress controller) is responsible for routing the requests to the appropriate **gateway deployment**. The **gateway deployment** then routes the requests to the **model service**, which is responsible for routing the requests to the **model deployment**.\n",
    "\n",
    "A service in Kubernetes is an abstraction that defines a logical set of pods, essentially serving as an entry point to a group of pods (deployment). A service ensures that the requests are routed to the appropriate pods within a deployment.\n",
    "\n",
    "<center><img src = \"figures/kubernets-response.png\" width=\"630\" height=\"300\"/></center>\n",
    "\n",
    "The response route after a request is made follows the reverse path. The **model deployment** processes the request and sends the response back to the **model service**. The **model service** routes the response to the **gateway deployment**. The **gateway deployment** passes the response to the **gateway service**, which then sends it to the **Ingress**. Finally, the **Ingress** routes the response back to the client."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Creating a cluster**\n",
    "\n",
    "Now let's create a cluster with `kind`:\n",
    "\n",
    "```bash\n",
    "kind create cluster\n",
    "```\n",
    "\n",
    "`Terminal`\n",
    "\n",
    "----\n",
    "```bash\n",
    "marcos@marcos:~$ sudo kind create cluster\n",
    "\n",
    "Creating cluster \"kind\" ...\n",
    " ‚úì Ensuring node image (kindest/node:v1.27.3) üñº \n",
    " ‚úì Preparing nodes üì¶  \n",
    " ‚úì Writing configuration üìú \n",
    " ‚úì Starting control-plane üïπÔ∏è \n",
    " ‚úì Installing CNI üîå \n",
    " ‚úì Installing StorageClass üíæ \n",
    "Set kubectl context to \"kind-kind\"\n",
    "You can now use your cluster with:\n",
    "\n",
    "kubectl cluster-info --context kind-kind\n",
    "\n",
    "Have a question, bug, or feature request? Let us know! https://kind.sigs.k8s.io/#community üôÇ\n",
    "\n",
    "```\n",
    "----\n",
    "\n",
    "And check with `kubectl` that it was successfully created:\n",
    "\n",
    "```bash\n",
    "kubectl cluster-info\n",
    "```\n",
    "\n",
    "`Terminal`\n",
    "\n",
    "----\n",
    "```bash\n",
    "marcos@marcos:~$ sudo kubectl cluster-info\n",
    "Kubernetes control plane is running at https://127.0.0.1:45357\n",
    "CoreDNS is running at https://127.0.0.1:45357/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n",
    "\n",
    "To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n",
    "```\n",
    "----\n",
    "\n",
    "## **Question 3**\n",
    "\n",
    "Now let's test if everything works. Use `kubectl` to get the list of running services. \n",
    "\n",
    "What's `CLUSTER-IP` of the service that is already running there? \n",
    "\n",
    "- **10.96.0.1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Terminal`\n",
    "\n",
    "----\n",
    "```bash\n",
    "marcos@marcos:~$ sudo kubectl get services\n",
    "[sudo] password for marcos: \n",
    "NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\n",
    "kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   39m\n",
    "```\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Question 4**\n",
    "\n",
    "To be able to use the docker image we previously created (`zoomcamp-model:hw10`),\n",
    "we need to register it with `kind`.\n",
    "\n",
    "What's the command we need to run for that?\n",
    "\n",
    "* `kind create cluster`\n",
    "* `kind build node-image`\n",
    "* **`kind load docker-image`**\n",
    "* `kubectl apply`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the docker images to ensure that the Docker image `zoomcamp-model:hw10` is available. \n",
    "\n",
    "\n",
    "`Terminal`\n",
    "\n",
    "----\n",
    "```bash\n",
    "marcos@marcos:~$ sudo docker images\n",
    "[sudo] password for marcos: \n",
    "REPOSITORY              TAG            IMAGE ID       CREATED        SIZE\n",
    "zoomcamp-model          hw10           4af085cd30a2   25 hours ago   432MB\n",
    "svizor/zoomcamp-model   3.10.12-slim   08266c8f0c4b   8 weeks ago    147MB\n",
    "```\n",
    "----\n",
    "\n",
    "With kind we created a local cluster by running each Kubernetes component as a separate Docker container. A local Docker network is then used to interconnect these containers, simulating the nodes of a Kubernetes cluster. With this approach, we can use the `kind load docker-image` command to load the Docker image into the local cluster.\n",
    "\n",
    "\n",
    "`Terminal`\n",
    "\n",
    "----\n",
    "    \n",
    "```bash\n",
    "marcos@marcos:~$ sudo kind load docker-image zoomcamp-model:hw10\n",
    "Image: \"zoomcamp-model:hw10\" with ID \"sha256:4af085cd30a2b4a3e7b3ede2a57bce241b8fd8508b117d671e6c59ead838e57f\" not yet present on node \"kind-control-plane\", loading...\n",
    "```\n",
    "----\n",
    "\n",
    "Once the Docker image is loaded into kind, it becomes available in the local Kubernetes cluster. To actually run the image, a Kubernetes deployment (group of pods) or pod needs to be created that references this image. This setup allows us to emulate a Kubernetes cluster locally, testing our application as if it were running in a real cluster environment.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **deployment and service**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Question 5**\n",
    "\n",
    "Now let's create a deployment config (e.g. `deployment.yaml`):\n",
    "\n",
    "```yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: credit\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: credit\n",
    "  replicas: 1\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: credit\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: credit\n",
    "        image: <Image>\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"64Mi\"\n",
    "            cpu: \"100m\"            \n",
    "          limits:\n",
    "            memory: <Memory>\n",
    "            cpu: <CPU>\n",
    "        ports:\n",
    "        - containerPort: <Port>\n",
    "```\n",
    "\n",
    "Replace `<Image>`, `<Memory>`, `<CPU>`, `<Port>` with the correct values.\n",
    "\n",
    "What is the value for `<Port>`?\n",
    "\n",
    "- **9696**\n",
    "\n",
    "Apply this deployment using the appropriate command and get a list of running Pods. \n",
    "You can see one running Pod.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A YAML file, standing for \"YAML Ain't Markup Language\". Is a human-readable data serialization format commonly used for configuration files and data exchange. Its cross-language compatibility and readability have made it a popular choice.\n",
    "\n",
    "\n",
    "`deployment.yaml`\n",
    "\n",
    "----\n",
    "\n",
    "```yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: credit\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: credit\n",
    "  replicas: 1\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: credit\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: credit\n",
    "        image: zoomcamp-model:hw10\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"64Mi\"\n",
    "            cpu: \"100m\"            \n",
    "          limits:\n",
    "            memory: \"256Mi\"\n",
    "            cpu: \"200m\"\n",
    "        ports:\n",
    "        - containerPort: 9696\n",
    "```\n",
    "----\n",
    "\n",
    "The `requests` field specifies the minimum CPU and memory required by the Pod, while the `limits` field specifies the maximum CPU and memory allowed by the Pod. The `containerPort` field specifies the port on which the container listens for requests.\n",
    "\n",
    "Now lets apply the deployment:\n",
    "\n",
    "`Termninal`\n",
    "\n",
    "----\n",
    "```bash\n",
    "marcos@marcos:~/GitHub/ML-zoomcamp/10kubernetes$ sudo kubectl apply -f deployment.yaml\n",
    "\n",
    "deployment.apps/credit created\n",
    "```\n",
    "----\n",
    "\n",
    "and check the deployment and pods:\n",
    "\n",
    "\n",
    "`Termninal`\n",
    "\n",
    "----\n",
    "```bash\n",
    "marcos@marcos:~$ sudo kubectl get deployment\n",
    "NAME     READY   UP-TO-DATE   AVAILABLE   AGE\n",
    "credit   1/1     1            1           32s\n",
    "\n",
    "marcos@marcos:~$ sudo kubectl get pods\n",
    "NAME                     READY   STATUS    RESTARTS   AGE\n",
    "credit-59d5ff45f-zjxg8   1/1     Running   0          46s\n",
    "```\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Question 6**\n",
    "\n",
    "Let's create a service for this deployment (`service.yaml`):\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: <Service name>\n",
    "spec:\n",
    "  type: LoadBalancer\n",
    "  selector:\n",
    "    app: <???>\n",
    "  ports:\n",
    "  - port: 80\n",
    "    targetPort: <PORT>\n",
    "```\n",
    "\n",
    "Fill it in. What do we need to write instead of `<???>`?\n",
    "\n",
    "Apply this config file.\n",
    "\n",
    "\n",
    "- **credit**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the `deployment.yaml` file, the label assigned to our pods is `app: credit`. In `service.yaml` file, we need to use this label in the `selector` to ensure the service routes traffic to these pods. The `name` of the service, `credit-service`, is used to identify the service within the Kubernetes cluster. The `targetPort` field specifies the port on which the service will send requests to the pods.\n",
    "\n",
    "`service.yaml`\n",
    "\n",
    "-----\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: credit\n",
    "spec:\n",
    "  type: LoadBalancer\n",
    "  selector:\n",
    "    app: credit\n",
    "  ports:\n",
    "  - port: 80\n",
    "    targetPort: 9696\n",
    "```\n",
    "-----\n",
    "\n",
    "Let's apply the service configuration and check:\n",
    "\n",
    "`Termninal`\n",
    "\n",
    "----\n",
    "```bash\n",
    "marcos@marcos:~/GitHub/ML-zoomcamp/10kubernetes$ sudo kubectl apply -f service.yaml\n",
    "service/credit-service created\n",
    "\n",
    "marcos@marcos:~$ sudo kubectl get services\n",
    "NAME             TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\n",
    "credit-service   LoadBalancer   10.96.206.120   <pending>     80:31813/TCP   74s\n",
    "kubernetes       ClusterIP      10.96.0.1       <none>        443/TCP        3h16m\n",
    "\n",
    "```\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Testing the service**\n",
    "\n",
    "We can test our service locally by forwarding the port 9696 on our computer \n",
    "to the port 80 on the service:\n",
    "\n",
    "```bash\n",
    "kubectl port-forward service/<Service name> 9696:80\n",
    "```\n",
    "\n",
    "Run `q6_test.py` (from the homework 5) once again to verify that everything is working. \n",
    "You should get the same result as in Question 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a request to the service running in the cluster, we need to forward the port 9696 on our computer to the port 80 on the service. This can be done using the `kubectl port-forward` command. The `service.yaml` file specifies that the service listens on port 80, while the `deployment.yaml` file specifies that the pods listen on port 9696. Therefore, we need to forward port 9696 on our computer to port 80 on the service:\n",
    "\n",
    "```bash\n",
    "marcos@marcos:~$ sudo kubectl port-forward service/credit 9696:80\n",
    "Forwarding from 127.0.0.1:9696 -> 9696\n",
    "\n",
    "Handling connection for 9696\n",
    "```\n",
    "\n",
    "Now, with the port forwarding in place, we can test the service's response by making requests to `http://localhost:9696/predict`, which will be routed to the credit-service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Credit probability': 0.726936946355423}\n"
     ]
    }
   ],
   "source": [
    "# Testing the cluster service response\n",
    "url = \"http://localhost:9696/predict\"\n",
    "client = {\"job\": \"retired\", \"duration\": 445, \"poutcome\": \"success\"}\n",
    "response = requests.post(url, json=client).json()\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Autoscaling**\n",
    "\n",
    "Now we're going to use a [HorizontalPodAutoscaler](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/) \n",
    "(HPA for short) that automatically updates a workload resource (such as our deployment), \n",
    "with the aim of automatically scaling the workload to match demand.\n",
    "\n",
    "Use the following command to create the HPA:\n",
    "\n",
    "```bash\n",
    "kubectl autoscale deployment credit --name credit-hpa --cpu-percent=20 --min=1 --max=3\n",
    "```\n",
    "\n",
    "You can check the current status of the new HPA by running:\n",
    "\n",
    "```bash\n",
    "kubectl get hpa\n",
    "```\n",
    "\n",
    "The output should be similar to the next:\n",
    "\n",
    "```bash\n",
    "NAME              REFERENCE                TARGETS   MINPODS   MAXPODS   REPLICAS   AGE\n",
    "credit-hpa   Deployment/credit   1%/20%    1         3         1          27s\n",
    "```\n",
    "\n",
    "`TARGET` column shows the average CPU consumption across all the Pods controlled by the corresponding deployment.\n",
    "Current CPU consumption is about 0% as there are no clients sending requests to the server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "first download the metrics-server and apply it:\n",
    "\n",
    "`Terminal`\n",
    "----\n",
    "\n",
    "```bash\n",
    "marcos@marcos:~/GitHub/ML-zoomcamp/10kubernetes$ wget https://raw.githubusercontent.com/pythianarora/total-practice/master/sample-kubernetes-code/metrics-server.yaml\n",
    "\n",
    "marcos@marcos:~/GitHub/ML-zoomcamp/10kubernetes$ sudo kubectl apply -f metrics-server.yaml\n",
    "\n",
    "\n",
    "serviceaccount/metrics-server created\n",
    "clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created\n",
    "clusterrole.rbac.authorization.k8s.io/system:metrics-server created\n",
    "rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created\n",
    "clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created\n",
    "clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created\n",
    "service/metrics-server created\n",
    "deployment.apps/metrics-server created\n",
    "apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created\n",
    "```\n",
    "----\n",
    "\n",
    "Autoscaling refers to the dynamic allocation of resources to a service based on current demand. It's used to maintain application performance and reduce costs. During times of high demand, the HPA will increase the number of Pods to prevent overloading and maintain performance. During times of low demand, the HPA will reduce the number of Pods to reduce costs. We can do this as follows:\n",
    "\n",
    "\n",
    "`Terminal`\n",
    "\n",
    "----\n",
    "```bash\n",
    "\n",
    "marcos@marcos:~$ sudo kubectl autoscale deployment credit --name credit-hpa --cpu-percent=20 --min=1 --max=3 \n",
    "\n",
    "horizontalpodautoscaler.autoscaling/credit-hpa autoscaled\n",
    "\n",
    "marcos@marcos:~$ sudo kubectl get hpa\n",
    "\n",
    "NAME         REFERENCE           TARGETS   MINPODS   MAXPODS   REPLICAS   AGE\n",
    "credit-hpa   Deployment/credit   1%/20%    1         3         1          3m5s\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "marcos@marcos:~$ sudo kubectl describe hpa credit-hpa\n",
    "\n",
    "Name:                                                  credit-hpa\n",
    "Namespace:                                             default\n",
    "Labels:                                                <none>\n",
    "Annotations:                                           <none>\n",
    "CreationTimestamp:                                     Mon, 04 Dec 2023 16:09:40 -0300\n",
    "Reference:                                             Deployment/credit\n",
    "Metrics:                                               ( current / target )\n",
    "  resource cpu on pods  (as a percentage of request):  1% (1m) / 20%\n",
    "Min replicas:                                          1\n",
    "Max replicas:                                          3\n",
    "Deployment pods:                                       1 current / 1 desired\n",
    "Conditions:\n",
    "\n",
    "  Type            Status  Reason              Message\n",
    "  ----            ------  ------              -------\n",
    "  AbleToScale     True    ReadyForNewScale    recommended size matches current size\n",
    "  ScalingActive   True    ValidMetricFound    the HPA was able to successfully calculate a replica ...\n",
    "  ScalingLimited  False   DesiredWithinRange  the desired count is within the acceptable range\n",
    "\n",
    "Events:           <none>\n",
    "\n",
    "```\n",
    "----\n",
    "\n",
    "\n",
    "The parameters `--cpu-percent=20`, `--min=1`, and `--max=3` indicate that the HPA should maintain CPU usage of the Pods at 20% of the requested CPU. If usage goes above that, the HPA will start creating new Pods (up to a maximum of 3). If the usage is low, it will reduce the number of Pods (but never below 1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Increase the load**\n",
    "\n",
    "Let's see how the autoscaler reacts to increasing the load. To do this, we can slightly modify the existing script by putting the operator that sends the request to the credit service into a loop.\n",
    "\n",
    "```python\n",
    "while True:\n",
    "    sleep(0.1)\n",
    "    response = requests.post(url, json=client).json()\n",
    "    print(response)\n",
    "```\n",
    "\n",
    "Now you can run this script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the HPA, we can artificially increase the load on our service to continuously send requests our service. This simulates high demand, prompting the HPA to scale out the number of Pods to maintain performance. Let's make the following coding running indefinitely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "url = \"http://localhost:9696/predict\"\n",
    "client = {\"job\": \"retired\", \"duration\": 445, \"poutcome\": \"success\"}\n",
    "while True:\n",
    "    sleep(0.1)\n",
    "    response = requests.post(url, json=client).json()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 7**\n",
    "\n",
    "Run `kubectl get hpa credit-hpa --watch` command to monitor how the autoscaler performs. \n",
    "Within a minute or so, you should see the higher CPU load; and then - more replicas. \n",
    "What was the maximum amount of the replicas during this test?\n",
    "\n",
    "* 1\n",
    "* **2**\n",
    "* 3\n",
    "* 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`Terminal`\n",
    "----\n",
    "\n",
    "```bash\n",
    "marcos@marcos:~$ kubectl port-forward service/credit 9696:80\n",
    "```\n",
    "\n",
    "\n",
    "```bash\n",
    "marcos@marcos:~$ sudo kubectl get hpa credit-hpa --watch\n",
    "NAME         REFERENCE           TARGETS   MINPODS   MAXPODS   REPLICAS   AGE\n",
    "credit-hpa   Deployment/credit   1%/20%    1         3         1          15m\n",
    "credit-hpa   Deployment/credit   15%/20%   1         3         1          16m\n",
    "....\n",
    "\n",
    "credit-hpa   Deployment/credit   32%/20%   1         3         2          16m\n",
    "\n",
    "....\n",
    "\n",
    "```\n",
    "----\n",
    "\n",
    "When the CPU load (TARGETS) exceeds the threshold (20%), the HPA will increase the number of replicas (REPLICAS) to distribute the load across more pods. The maximum number of replicas it can scale to is determined by MAXPODS. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
