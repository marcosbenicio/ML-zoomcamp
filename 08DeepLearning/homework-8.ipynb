{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from scipy.ndimage import convolve\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Layer, Conv2D, MaxPool2D, Flatten\n",
    "from keras.losses import Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "# **Convolutional Neural Networks (CNNs) - Theory**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Convolutional Neural Network (CNN) is essentially a feedforward Multi-Layer Perceptron (MLP) that is designed to recognize local patterns and sparsity in input data. Like the MLP, each neuron is connected to others through learnable weights. These weights are adjusted during training to optimize the network's performance for a specific task.\n",
    "\n",
    "The main difference between MLPs and CNNs is that the latter is developed for processing multidimensional data, such as images or videos. Also, CNNs have a more diverse set of specialized layers, including convolutional layers, pooling layers, and upsampling layers, which are optimized for processing spatial (image) and temporal data (video)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Convolutions**\n",
    "\n",
    "Convolution is a mathematical operation that involves sliding a small matrix, known as a kernel or filter, across a larger matrix representing the input data, such as an image. During this process, the element-wise product $\\odot$ is computed between the kernel and each local region (sub-matrix) it covers on the input data matrix. The result of this operation is a new matrix, called a feature map, which encodes information about the presence, absence, or strength of specific features in the input data.\n",
    "\n",
    "Let's examine the following convolutional operations to illustrate this concept.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **One Dimension**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider $\\mathbf{x}$ as an input vector with $n$ elements and $\\mathbf{w}$ as a weight vector, also known as a  **filter**, with $k \\leq n$.\n",
    "\n",
    "$$\n",
    "\\mathbf{x} =\n",
    "\\left( \\begin{array}{c}\n",
    "x_{1}\\\\\n",
    "x_{2}\\\\\n",
    "\\vdots\\\\\n",
    "x_{n}\n",
    "\\end{array} \\right), ~~~~\n",
    "\\mathbf{w} =\n",
    "\\left( \\begin{array}{c}\n",
    "w_{1}\\\\\n",
    "w_{2}\\\\\n",
    "\\vdots\\\\\n",
    "w_{k}\n",
    "\\end{array} \\right)\n",
    "$$\n",
    "\n",
    "Here $k$  is known as the  **window size** and indicates the size of the filter applied to the input vector $\\mathbf{x}$. It defines the region of the local neighborhood within the input vector $\\mathbf{x}$ used for computing output values. To proceed, we define a subvector of $\\mathbf{x}$ with the same size as the filter vector. Let $\\mathbf{x}_k(i)$ denote the window of $\\mathbf{x}$  of size $k$ starting at position $i$:\n",
    "\n",
    "$$\\mathbf{x}_k(i) = \\left( \\begin{array}{c}\n",
    "x_i \\\\\n",
    "x_{i+1} \\\\\n",
    "\\vdots\\\\\n",
    "x_{i+k-1} \n",
    "\\end{array} \\right).$$\n",
    "\n",
    "For $k \\leq n$, it must be that $i+k-1 \\leq n$, implying $1 \\leq i \\leq n-k+1$. As a validity test, if we start at $i =  n-k+1$, then the end position is $i+k-1 = n$. If we calculate the total number of elements by the difference in position provides the window size $k$, confirmed by $n - i = n - (n-k+1) = k$. For example, with $n = 5$ and $k = 3$:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} =\n",
    "\\left( \\begin{array}{c}\n",
    "x_{1}\\\\\n",
    "x_{2}\\\\\n",
    "x_{3}\\\\\n",
    "x_{4}\\\\\n",
    "x_{5}\n",
    "\\end{array} \\right), ~~~~\n",
    "\\mathbf{w} =\n",
    "\\left( \\begin{array}{c}\n",
    "w_{1}\\\\\n",
    "w_{2}\\\\\n",
    "w_{3}\n",
    "\\end{array} \\right)\n",
    "$$\n",
    "\n",
    "the window of $\\mathbf{x}$ from $i = 2$ to $i+k-1 = 4$ is:\n",
    "\n",
    "$$\\mathbf{x}_3(2) = \\left( \\begin{array}{c}\n",
    "x_2 \\\\\n",
    "x_{3}\\\\\n",
    "x_{4}\n",
    "\\end{array} \\right)$$\n",
    "\n",
    "**Example**\n",
    "\n",
    "Let's first consider a particular example with input vector $\\mathbf{x}$ of size $n = 5$ and a weight vector with window size $k = 3$. The vectors are illustrated in the following figure:\n",
    "\n",
    "<center><img src = \"figures/1d-conv.png\" width=\"800\" height=\"400\"/></center>\n",
    "\n",
    "\n",
    "The convolution steps for the sliding windows of $\\mathbf{x}$ with the filter $\\mathbf{w}$ are:\n",
    "\n",
    "\n",
    "$$\n",
    "\\sum \\mathbf{x}_3(1) \\odot \\mathbf{w} = \\sum (1, 3, -1)^T \\odot (1, 0, 2)^T = \\sum  (1 \\cdot 1, 3 \\cdot 0, -1 \\cdot 2) = 1 + 0 - 2 = -1,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum \\mathbf{x}_3(2) \\odot \\mathbf{w} = \\sum (3, -1, 2)^T \\odot (1, 0, 2)^T = \\sum  (3 \\cdot 1, -1 \\cdot 0, 2 \\cdot 2) = 3 + 0 + 4 = 7,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum \\mathbf{x}_3(3) \\odot \\mathbf{w} = \\sum (-1, 2, 3)^T \\odot (1, 0, 2)^T = \\sum  (-1 \\cdot 1, 2 \\cdot 0, 3 \\cdot 2) = -1 + 0 + 6 = 5.\n",
    "$$\n",
    "\n",
    "The element-wise product $\\odot$  , also known as the Hadamard product, multiplies corresponding elements in two vectors. Unlike the typical inner product, which multiplies an element by a column, this operation multiplies an element by its corresponding element in another vector. This steps provide the convolution between the two vectors resulting in a vector of size n-k+1 = 3. Thus, the convolution $\\mathbf{x} * \\mathbf{w}$ is:\n",
    "\n",
    "$$\\mathbf{x} * \\mathbf{w} =\n",
    "\\left( \\begin{array}{c}\n",
    "-1\\\\\n",
    "7\\\\\n",
    "5\n",
    "\\end{array} \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vector X: (5,)\n",
      "filter W: (3,)\n",
      "output X*W: [-1  7  5]\n"
     ]
    }
   ],
   "source": [
    "# Code for the example\n",
    "\n",
    "X = np.array([1, 3, -1, 2, 3])\n",
    "# flip the filter W to use the convolve function\n",
    "# as expected in machine learning and deep learning context\n",
    "W = np.flip(np.array([1, 0, 2]))\n",
    "\n",
    "# perform 1D convolution\n",
    "output = np.convolve(X, W, mode='valid')\n",
    "\n",
    "print(\"Input vector X:\", X.shape)\n",
    "print(\"filter W:\", W.shape)\n",
    "print(\"output X*W:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demonstrates that convolution is an element-wise product between a subvector and a weight vector of the same size, providing a scalar value when summed, which forms the result of the convolution operation.\n",
    "\n",
    "To simplify the notation, let's adopt the convention that for a vector $\\mathbf{a} \\in \\mathbb{R}^k$, define the summation operator as one that adds all elements of the vector. That is, \n",
    "\n",
    "$$\\text{Sum}(\\mathbf{a}) = \\sum_{i=1}^{k} a_{i}$$ \n",
    "\n",
    "Then from the example, we would have\n",
    "\n",
    "$$\n",
    "\\sum \\mathbf{x}_3(1) \\odot \\mathbf{w} = \\text{Sum}\\bigg( \\mathbf{x}_3(1) \\odot \\mathbf{w} \\bigg)= 1 + 0 - 2 = -1.\n",
    "$$\n",
    "\n",
    "Then, we can define a general one dimensional convolution operation between $\\mathbf{x}$ and $\\mathbf{w}$, denoted by the asterisk symbol $\\ast$, as \n",
    "\n",
    "$$\\mathbf{x} \\ast \\mathbf{w} = \\left( \\begin{array}{c}\n",
    "\\text{Sum}(\\mathbf{x}_k(1) \\odot \\mathbf{w})\\\\\n",
    "\\vdots\\\\\n",
    "\\text{Sum}(\\mathbf{x}_k(i) \\odot \\mathbf{w})\\\\\n",
    "\\vdots\\\\\n",
    "\\text{Sum}(\\mathbf{x}_k(n-k+1) \\odot \\mathbf{w})\n",
    "\\end{array} \\right).$$\n",
    "\n",
    "The convolution of $\\mathbf{x} \\in \\mathbf{R}^{n}$ and $\\mathbf{W} \\in \\mathbf{R}^{k}$ results in a vector of size $n-k+1$. The i-th element from this output vector can be decomposed as\n",
    "\n",
    "$$\\text{Sum}(\\mathbf{x_k}(i) \\odot \\mathbf{w}) = x_{i}w_1 + x_{i+1}w_2 + \\cdots + x_{(i+k-1)}w_k =  \\sum_{j=1}^{k} x_{(i+j-1)}w_j.$$\n",
    "\n",
    "This shows that the sum is over all elements of the subvector $\\mathbf{x}_k(i)$, so the last element of this sum must coincide with the last elements of $\\mathbf{x}_k(i)$ and $\\mathbf{w}$. This results in the convolution of $\\mathbf{x}$ with $\\mathbf{w}$ over the window defined by $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Two Dimension**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extend the convolution operation to an matrix input instead of a vector. Let $\\mathbf{X}$ be an input matrix with $n \\times n$ elements and $\\mathbf{W}$ be the weight matrix, also known as a  **filter**, with $k \\leq n$.\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\begin{bmatrix}\n",
    "x_{1,1} & x_{1,2} & \\cdots & x_{1,n} \\\\\n",
    "x_{2,1} & x_{2,2} & \\cdots & x_{2,n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{n,1} & x_{n,2} & \\cdots & x_{n,n}\n",
    "\\end{bmatrix},~~\n",
    "\\mathbf{W}=\\begin{bmatrix}\n",
    "w_{1,1} & w_{1,2} & \\cdots & w_{1,k} \\\\\n",
    "w_{2,1} & w_{2,2} & \\cdots & w_{2,k} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{k,1} & w_{k,2} & \\cdots & w_{k,k}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Here, similar to the one dimensional case,  $k$ is the window size and indicates the size of the filter applied to the input matrix $\\mathbf{X}$. From the one dimensional case we can extend the notion of a sub vector to a sub matrix. Let $\\mathbf{X}_k(i,j)$ denote the $k \\times k$ submatrix of $\\mathbf{X}$ starting at row $i$ and column $j$ as\n",
    "\n",
    "$$\\mathbf{X}_k(i,j) = \\begin{bmatrix}\n",
    "x_{i,j} & x_{i,~(j+1)} & \\cdots & x_{i,~(j+k-1)} \\\\\n",
    "x_{(i+1),~j} & x_{(i+2),~j} & \\cdots & x_{(i+1), (j+k-1)} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{(i+k-1), ~j} & x_{(i+1),(j+1)} & \\cdots & x_{(i+k-1),(j+k-1)}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "where for two indices, give that this is a square matrix, the range is simple $1 \\leq (i,j) \\leq n-k+1$.\n",
    "\n",
    "As for the one dimensional case, to simplify the notation, we adopt the convention that for a matrix $\\mathbf{A} \\in \\mathbb{R}^{k \\times k}$ define the summation operator as one that adds all elements of the matrix.\n",
    "\n",
    "$$\\text{Sum}(\\mathbf{A}) = \\sum_{i=1}^{k}\\sum_{j=1}^{k} a_{i,j}$$\n",
    "\n",
    "Then, we can define  a general two dimensional convolution operation between matrices $\\mathbf{X}$ and $\\mathbf{W}$, as\n",
    "\n",
    "\n",
    "$$\\mathbf{X} \\ast \\mathbf{W} = \\begin{bmatrix}\n",
    "\\text{Sum}(x_k(1,1) \\odot \\mathbf{W}) &\\text{Sum}(x_k(1,2) \\odot \\mathbf{W}) & \\cdots & \\text{Sum}(x_k(1,n-k+1) \\odot \\mathbf{W}) \\\\\n",
    "\\text{Sum}(x_k(2,1) \\odot \\mathbf{W}) & \\text{Sum}(x_k(2,2) \\odot \\mathbf{W}) & \\cdots &\\text{Sum}(x_k(2,n-k+1) \\odot \\mathbf{W})\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\text{Sum}(x_k(n-k+1,1) \\odot \\mathbf{W}) & \\text{Sum}(x_k(n-k+1,2) \\odot \\mathbf{W}) & \\cdots & \\text{Sum}(x_k(n-k+1,n-k+1) \\odot \\mathbf{W})\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\text{Sum}(\\mathbf{X}_k(i,j) \\odot \\mathbf{W})=\\sum_{a=1}^{k}\\sum_{b=1}^{k} x_{(i+a-1),(j+b-1)} w_{a,b}$$\n",
    "\n",
    "The convolution of $\\mathbf{X} \\in \\mathbf{R}^{n \\times n}$ and $\\mathbf{W} \\in \\mathbf{R}^{k \\times k}$ results in a $(n-k+1) \\times (n-k+1)$ matrix.\n",
    "\n",
    "**Example**\n",
    "\n",
    "Let's consider a particular example with input matrix $\\mathbf{X}$ with dimension  $3 \\times 3$ (n = 3) and a weight matrix with dimension  $2 \\times 2$ (k = 2). The matrices are illustrated in the following figure:\n",
    "\n",
    "<center><img src = \"figures/2d-conv.png\" width=\"800\" height=\"400\"/></center>\n",
    "\n",
    "The convolution steps for the sliding windows of $\\mathbf{X}$ with the filter $\\mathbf{W}$ illustrated in the figure are mathematically translated to:\n",
    "\n",
    "\n",
    "$$\\text{Sum}(\\mathbf{X}_k(1,1) \\odot \\mathbf{W})=\\text{Sum}\\bigg(\n",
    "    \\begin{bmatrix} \n",
    "    1 & 2 \\\\\n",
    "    3 & 1 \n",
    "    \\end{bmatrix} \n",
    "    \\odot \n",
    "    \\begin{bmatrix} \n",
    "    1 & 0 \\\\\n",
    "    0 & 1\n",
    "    \\end{bmatrix} \\bigg) =  2$$\n",
    "\n",
    "$$\\text{Sum}(\\mathbf{X}_2(1,2) \\odot \\mathbf{W}) = \\text{Sum}\\bigg(\n",
    "    \\begin{bmatrix} \n",
    "    2 & 2 \\\\\n",
    "    1 & 4\n",
    "    \\end{bmatrix} \n",
    "    \\odot\n",
    "    \\begin{bmatrix}\n",
    "    1 & 0 \\\\\n",
    "    0 & 1\n",
    "    \\end{bmatrix} \\bigg)= 6$$\n",
    "\n",
    "\n",
    "$$\\text{Sum}(\\mathbf{X}_2(2,1) \\odot \\mathbf{W}) = \\text{Sum}\\bigg( \n",
    "    \\begin{bmatrix} \n",
    "    3 & 1 \\\\ \n",
    "    2 & 1 \\end{bmatrix} \n",
    "    \\odot \\begin{bmatrix} \n",
    "    1 & 0 \\\\ \n",
    "    0 & 1 \n",
    "    \\end{bmatrix} \\bigg)= 4$$\n",
    "\n",
    "\n",
    "$$\\text{Sum}(\\mathbf{X}_2(2,2) \\odot \\mathbf{W}) = \\text{Sum}\\bigg( \n",
    "    \\begin{bmatrix} \n",
    "    1 & 4 \\\\\n",
    "    3 & 3 \\end{bmatrix}\n",
    "    \\odot\n",
    "    \\begin{bmatrix}\n",
    "    1 & 0 \\\\ \n",
    "    0 & 1\n",
    "    \\end{bmatrix} \\bigg) = 4$$\n",
    "\n",
    "The convolution $\\mathbf{X}*\\mathbf{W}$ has size $2 \\times 2$, since  $n - k + 1 = 3 - 2 + 1 = 2$, and is given by\n",
    "\n",
    "$$\\mathbf{X}*\\mathbf{W} = \n",
    "    \\begin{bmatrix} \n",
    "        \\text{Sum}(\\mathbf{X}_2(1,1) \\odot \\mathbf{W}) & \\text{Sum}(\\mathbf{X}_2(1,2) \\odot \\mathbf{W}) \\\\\n",
    "        \\\\\n",
    "        \\text{Sum}(\\mathbf{X}_2(2,1) \\odot \\mathbf{W}) & \\text{Sum}(\\mathbf{X}_2(2,2) \\odot \\mathbf{W}) \n",
    "    \\end{bmatrix} = \n",
    "    \\begin{bmatrix} \n",
    "        2 & 6 \\\\\n",
    "        4 & 4 \n",
    "    \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Three dimensional Convolution on CNNs**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now extend the convolution operation to a three-dimensional matrix, also called a rank-3 tensor. The first dimension comprises the rows (height), second dimension the columns (width) and the third dimension the channels (number of 2D slices stacked along the depth axis). Typically in CNNs, we use a 3D filter $\\mathbf{W} \\in \\mathbb{R}^{k \\times k \\times m}$, with the number of channels equal to the number of channels of the input tensor $\\mathbf{X} \\in \\mathbb{R}^{n \\times n \\times m}$, in this case with $m$ channels each. Mathematically we represent as:\n",
    "\n",
    "$$\n",
    "\\mathbf{X}=\n",
    "\\begin{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_{1,1,1} & x_{1,2,1} & \\cdots & x_{1,n,1} \\\\\n",
    "x_{2,1,1} & x_{2,2,1} & \\cdots & x_{2,n,1} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{n,1,1} & x_{n,2,1} & \\cdots & x_{n,n,1}\n",
    "\\end{bmatrix}\\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "x_{1,1,2} & x_{1,2,2} & \\cdots & x_{1,n,2} \\\\\n",
    "x_{2,1,2} & x_{2,2,2} & \\cdots & x_{2,n,2} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{n,1,2} & x_{n,2,2} & \\cdots & x_{n,n,2}\n",
    "\\end{bmatrix}\\\\\n",
    "\\\\\n",
    "\\vdots\\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "x_{1,1,m} & x_{1,2,m} & \\cdots & x_{1,n,m} \\\\\n",
    "x_{2,1,m} & x_{2,2,m} & \\cdots & x_{2,n,m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{n,1,m} & x_{n,2,m} & \\cdots & x_{n,n,m}\n",
    "\\end{bmatrix}\n",
    "\\end{bmatrix},~~\n",
    "\n",
    "\n",
    "\\mathbf{W}= \\begin{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "w_{1,1,1} & w_{1,2,1} & \\cdots & w_{1,k,1} \\\\\n",
    "w_{2,1,1} & w_{2,2,1} & \\cdots & w_{2,k,1} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{k,1,1} & w_{k,2,1} & \\cdots & w_{k,k,1}\n",
    "\\end{bmatrix}\\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "w_{1,1,2} & w_{1,2,2} & \\cdots & w_{1,k,2} \\\\\n",
    "w_{2,1,2} & w_{2,2,2} & \\cdots & w_{2,k,2} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{k,1,2} & w_{k,2,2} & \\cdots & w_{k,k,2}\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "\\vdots\\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "w_{1,1,r} & w_{1,2,r} & \\cdots & w_{1,k,m} \\\\\n",
    "w_{2,1,r} & w_{2,2,r} & \\cdots & w_{2,k,m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{k,1,r} & w_{k,2,r} & \\cdots & w_{k,k,m}\n",
    "\n",
    "\\end{bmatrix}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Similar to convolutions in other dimensions, the window size must satisfy $k \\leq n$, and the total number of slice matrices along the depth of the filter and input tensor are fixed as $m$. This mathematical representation of a tensor may seem complex at first, but it closely resembles how Python libraries like NumPy represent a tensor, with exception of the order of rows, columns and depth.\n",
    "\n",
    "When defining a sub-tensor from the input tensor $\\mathbf{X}$, let $\\mathbf{X}_k(i,j)$ denote a $k \\times k \\times m$ sub-tensor of $\\mathbf{X}$ that starts at row $i$, column $j$, and encompasses the full depth $m$ of the input. In the context where the filter spans the entire depth of the input (i.e., the number of channels $r$ in the filter is equal to the depth $m$ of the input), the depth index $q$ is not needed because the filter processes all depth layers simultaneously. Therefore, the sub-tensor $\\mathbf{X}_k(i,j)$ is defined as follows:\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{X}_k(i,j,q = m)= \\mathbf{X}_k(i,j) =\n",
    "\\begin{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_{i,j,1} & x_{i,(j+1),1} & \\cdots & x_{i,(j+k-1),1} \\\\\n",
    "x_{(i+1),j,1} & x_{(i+1),(j+1),1} & \\cdots & x_{(i+1),(j+k-1),1} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{(i+k-1),j,1} & x_{(i+k-1),(j+1),1} & \\cdots & x_{(i+k-1),(j+k-1),1}\n",
    "\\end{bmatrix}\\\\\n",
    "\\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "x_{i,j,2} & x_{i,(j+1),2} & \\cdots & x_{i,(j+k-1),2} \\\\\n",
    "x_{(i+1),j,2} & x_{(i+1),(j+1),2} & \\cdots & x_{(i+1),(j+k-1),2} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{(i+k-1),j,2} & x_{(i+k-1),(j+1),2} & \\cdots & x_{(i+k-1),(j+k-1),2}\n",
    "\\end{bmatrix}\\\\\n",
    "\\\\\n",
    "\\vdots\\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "x_{i,j,m} & x_{i,(j+1),m} & \\cdots & x_{i,(j+k-1),m} \\\\\n",
    "x_{(i+1),j,m} & x_{(i+1),(j+1),m} & \\cdots & x_{2,n,m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{(i+k-1),j,m} & x_{(i+k-1),(j+1),m} & \\cdots & x_{(i+k-1),(j+k-1),m}\n",
    "\\end{bmatrix}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The indices from the subtensor for rows and columns range from $1 \\leq (i,j) \\leq n-k+1$, where $n$ is the dimension of the input $\\mathbf{X}$ and $k$ is the window size of the filter $\\mathbf{W}$, consistent with two-dimensional convolutions. The depth dimension is fixed at $m$, so it's redundant carrying $q = m$ in our notation. \n",
    "\n",
    "As in the case of convolution in lower dimensions, we define the summation operator as one that adds all elements within the tensor. Therefore, given a tensor $\\mathbf{A} \\in \\mathbb{R}^{k \\times k \\times m}$, the summation operation is defined as:\n",
    "\n",
    "$$\\text{Sum}(\\mathbf{A}) = \\sum_{a=1}^{k}\\sum_{b=1}^{k}\\sum_{q=1}^{m}a_{ijq}$$\n",
    "\n",
    "This summation adds up all the elements in the tensor $\\mathbf{A}$, where $a_{ijq}$ denotes the element located at the $i$-th row, $j$-th column, and $q$-th depth layer.\n",
    "\n",
    "\n",
    "Before we generalize the convolution operation to three dimensions, let's consider an example to illustrate the logic behind this mathematical notation and how it translates to the operations performed in a CNN.\n",
    "\n",
    "**Example**\n",
    "\n",
    "Consider a input tensor $\\mathbf{X}$ with dimension  $3 \\times 3 \\times 3$ (n = 3 and m = 3 channels) and a filter with dimension  $2 \\times 2 \\times 3$ ( windows size with k = 2 and m = 3 channels). The tensors are illustrated in the following figure:\n",
    "\n",
    "<center><img src = \"figures/3d-conv.png\" ></center>\n",
    "\n",
    "\n",
    "The convolution steps for the sliding windows of $\\mathbf{X}$ with the filter $\\mathbf{W}$ illustrated in the figure are:\n",
    "\n",
    "$$\n",
    "\\text{Sum}(\\mathbf{X}_2(1,1) \\odot \\mathbf{W}) = \\text{Sum}\\bigg( \n",
    "\\begin{bmatrix} \n",
    "    \\begin{bmatrix} \n",
    "        1 & -1 \\\\ \n",
    "        2 & 1 \n",
    "    \\end{bmatrix} \\\\\n",
    "    \\\\\n",
    "    \\begin{bmatrix} \n",
    "        2 & 1 \\\\\n",
    "        3 & -1\n",
    "    \\end{bmatrix} \\\\\n",
    "    \\\\\n",
    "    \\begin{bmatrix} \n",
    "        1 & -2 \\\\ \n",
    "        2 & 1 \n",
    "    \\end{bmatrix} \n",
    "\\end{bmatrix} \n",
    "\\odot \n",
    "\\begin{bmatrix} \n",
    "    \\begin{bmatrix} \n",
    "        1 & 1 \\\\\n",
    "        2 & 0 \n",
    "    \\end{bmatrix} \\\\\n",
    "    \\\\\n",
    "    \\begin{bmatrix} \n",
    "        1 & 0 \\\\ \n",
    "        0 & 1\n",
    "    \\end{bmatrix} \\\\\n",
    "    \\\\\n",
    "    \\begin{bmatrix}\n",
    "        0 & 1 \\\\\n",
    "        1 & 0 \n",
    "    \\end{bmatrix} \n",
    "\\end{bmatrix} \\bigg) = \\text{Sum}\\bigg(   \n",
    "    \\begin{bmatrix} \n",
    "    \\begin{bmatrix} \n",
    "        1  & -1  \\\\ \n",
    "        4 & 0 \n",
    "    \\end{bmatrix} \\\\\n",
    "    \\\\\n",
    "    \\begin{bmatrix} \n",
    "        2 & 0 \\\\\n",
    "        0 & -1\n",
    "    \\end{bmatrix} \\\\\n",
    "    \\\\\n",
    "    \\begin{bmatrix} \n",
    "        0 & -2 \\\\ \n",
    "        2 & 0 \n",
    "    \\end{bmatrix} \n",
    "\\end{bmatrix} \\bigg) = 1 - 1 + 4 +2 -1 -2+ 2 = 5 \n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Sum}(\\mathbf{X}_2(1,2) \\odot \\mathbf{W}) = \\text{Sum}\\bigg(\n",
    "\\begin{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        -1 & 3 \\\\\n",
    "        1 & -4\n",
    "    \\end{bmatrix} \\\\\n",
    "    \\\\\n",
    "    \\begin{bmatrix}\n",
    "        1 & 3 \\\\\n",
    "        -1 & 1\n",
    "    \\end{bmatrix} \\\\\n",
    "    \\\\\n",
    "    \\begin{bmatrix}\n",
    "        -2 & 4 \\\\\n",
    "        1 & -2\n",
    "    \\end{bmatrix}\n",
    "\\end{bmatrix}\n",
    "\\odot\n",
    "\\begin{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        1 & 1 \\\\\n",
    "        2 & 0\n",
    "    \\end{bmatrix} \\\\\n",
    "    \\\\\n",
    "    \\begin{bmatrix}\n",
    "        1 & 0 \\\\\n",
    "        0 & 1\n",
    "    \\end{bmatrix} \\\\\n",
    "    \\\\\n",
    "    \\begin{bmatrix}\n",
    "        0 & 1 \\\\\n",
    "        1 & 0\n",
    "    \\end{bmatrix}\n",
    "\\end{bmatrix} \\bigg) = \\text{Sum}\\bigg(\n",
    "\\begin{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    -1 & 3 \\\\\n",
    "    2 & 0\n",
    "\\end{bmatrix} \\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "    1 & 0 \\\\\n",
    "    0 & 1\n",
    "\\end{bmatrix} \\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "    0 & 4 \\\\\n",
    "    1 & 0\n",
    "\\end{bmatrix}\n",
    "\\end{bmatrix} \\bigg) = -1 + 3 + 2 + 1 + 1 + 4 + 1 = 11 \n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Sum}(\\mathbf{X}_2(2,1) \\odot \\mathbf{W}) = \\text{Sum}\\bigg(\n",
    "\\begin{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    2 & 1 \\\\\n",
    "    3 & 1\n",
    "\\end{bmatrix} \\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "    3 & -1 \\\\\n",
    "    1 & 1\n",
    "\\end{bmatrix} \\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "    2 & 1 \\\\\n",
    "    1 & 3\n",
    "\\end{bmatrix}\n",
    "\\end{bmatrix}\n",
    "\\odot\n",
    "\\begin{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    1 & 1 \\\\\n",
    "    2 & 0\n",
    "\\end{bmatrix} \\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "    1 & 0 \\\\\n",
    "    0 & 1\n",
    "\\end{bmatrix} \\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "    0 & 1 \\\\\n",
    "    1 & 0\n",
    "\\end{bmatrix}\n",
    "\\end{bmatrix} \\bigg) = \\text{Sum}\\bigg(\n",
    "\\begin{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    2 & 1 \\\\\n",
    "    6 & 0\n",
    "\\end{bmatrix} \\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "    3 & 0 \\\\\n",
    "    0 & 1\n",
    "\\end{bmatrix} \\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "    0 & 1 \\\\\n",
    "    1 & 0\n",
    "\\end{bmatrix}\n",
    "\\end{bmatrix} \\bigg) = 2 + 1 + 6 + 3 + 1 + 1 + 1 = 15\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Sum}(\\mathbf{X}_2(2,2) \\odot \\mathbf{W}) = \\text{Sum}\\bigg(\n",
    "\\begin{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    1 & 4 \\\\\n",
    "    1 & 2\n",
    "\\end{bmatrix} \\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "    -1 & 1 \\\\\n",
    "    1 & -2\n",
    "\\end{bmatrix} \\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "    1 & -2 \\\\\n",
    "    3 & -1\n",
    "    \\end{bmatrix}\n",
    "\\end{bmatrix}\n",
    "\\odot\n",
    "\\begin{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    1 & 1 \\\\\n",
    "    2 & 0\n",
    "\\end{bmatrix} \\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "    1 & 0 \\\\\n",
    "    0 & 1\n",
    "\\end{bmatrix} \\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "    0 & 1 \\\\\n",
    "    1 & 0\n",
    "\\end{bmatrix}\n",
    "\\end{bmatrix} \\bigg) = \\text{Sum}\\bigg(\n",
    "\\begin{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    1 & 4 \\\\\n",
    "    2 & 0\n",
    "\\end{bmatrix} \\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "    -1 & 0 \\\\\n",
    "    0 & -2\n",
    "\\end{bmatrix} \\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "    0 & -2 \\\\\n",
    "    3 & 0\n",
    "\\end{bmatrix}\n",
    "\\end{bmatrix} \\bigg) = 1 + 4 +  2 + -1 - 2 - 2 + 3  = 5 \n",
    "$$\n",
    "\n",
    "The convolution $\\mathbf{X}*\\mathbf{W}$ has size $2 \\times 2$, since  $n - k + 1 = 3 - 2 + 1 = 2$, and $m = 3$; it is is given as\n",
    "\n",
    " $$\\mathbf{X}*\\mathbf{W} = \n",
    " \\begin{bmatrix}\n",
    "    \\text{Sum}(\\mathbf{X}_2(1,1) \\odot \\mathbf{W}) & \\text{Sum}(\\mathbf{X}_2(1,2) \\odot \\mathbf{W}) \\\\\n",
    "    \\\\\n",
    "    \\text{Sum}(\\mathbf{X}_2(2,1) \\odot \\mathbf{W}) & \\text{Sum}(\\mathbf{X}_2(2,2) \\odot \\mathbf{W})\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "    5 & 11 \\\\\n",
    "    15 & 5\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input tensor X:\n",
      " (1, 3, 3, 3, 1)\n",
      "Shape of filter tensor W:\n",
      " (3, 2, 2, 1, 1)\n",
      "Convolved output shape with channel and batch dimension:\n",
      " (1, 1, 2, 2, 1)\n",
      "Convolved output:\n",
      " [[ 5. 11.]\n",
      " [15.  5.]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([\n",
    "    [[1, -1, 3],\n",
    "     [2, 1, 4],\n",
    "     [3, 1, 2]],\n",
    "\n",
    "    [[2, 1, 3],\n",
    "     [3, -1, 1],\n",
    "     [1, 1, -2]],\n",
    "\n",
    "    [[1, -2, 4],\n",
    "     [2, 1, -2],\n",
    "     [1, 3, -1]]\n",
    "], dtype=np.float32)\n",
    "\n",
    "# TensorFlow expects the input to have a shape of [batch, (depth, height, width), channels]\n",
    "# Add a batch dimension and a channel dimension to X\n",
    "X = X.reshape(1, *X.shape, 1) \n",
    "# create a simple 3D kernel\n",
    "W = np.array([\n",
    "    [[1, 1],\n",
    "     [2, 0]],\n",
    "\n",
    "    [[1, 0],\n",
    "     [0, 1]],\n",
    "\n",
    "    [[0, 1],\n",
    "     [1, 0]]\n",
    "], dtype=np.float32)\n",
    "\n",
    "# TensorFlow expects the filter to have a shape of [(depth, height, width), in_channels = 1, out_channels = 1]\n",
    "# Since our input has a single channel (in_channels = 1) and we want a single output channel ( out_channels = 1),\n",
    "# Add those dimensions to W\n",
    "W = W.reshape(*W.shape, 1, 1) \n",
    "\n",
    "# 3D convolution\n",
    "output = tf.nn.conv3d(input=X, filters=W, strides=[1, 1, 1, 1, 1], padding=\"VALID\")\n",
    "\n",
    "# squeeze to remove the redundant dimensions of batch and channel\n",
    "output_2d = output.numpy().squeeze()    \n",
    "\n",
    "print(\"Shape of input tensor X:\\n\", X.shape)\n",
    "print(\"Shape of filter tensor W:\\n\", W.shape)\n",
    "print(\"Convolved output shape with channel and batch dimension:\\n\", output.shape)\n",
    "print(\"Convolved output:\\n\", output_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the number of channels is fixed for both tensors the result of the convolution is a matrix of two dimension, and not a tensor. The matrix from the convolution has dimension $(n-k+1) \\times (n-k+1)$ and can be represented as:\n",
    "\n",
    "$$\n",
    "\\mathbf{X} \\ast \\mathbf{W} = \n",
    "\\begin{bmatrix}\n",
    "\\text{Sum}(X_k(1,1) \\odot \\mathbf{W})  & \\cdots & \\text{Sum}(X_k(1,n-k+1) \\odot \\mathbf{W}) \\\\\n",
    "\\vdots  & \\ddots & \\vdots \\\\\n",
    "\\text{Sum}(X_k(n-k+1,1) \\odot \\mathbf{W}) & \\cdots & \\text{Sum}(X_k(n-k+1,n-k+1) \\odot \\mathbf{W})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Sum}(\\mathbf{X}_k(i,j) \\odot \\mathbf{W}) = \\sum_{a=1}^{k}\\sum_{b=1}^{k}\\sum_{c=1}^{m}x_{(i+a-1),(j+b-1),~c}~w_{a,b,c}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "for $(i,j) = 1,2, \\cdots, n-k+1$.\n",
    "\n",
    "\n",
    "This process can be visualized as each slice of the filter $\\mathbf{W}$ matching with a corresponding slice in $\\mathbf{X}$, aggregating information across all channels to form a 2D matrix that represents the features extracted from the input tensor $\\mathbf{X}$.\n",
    "\n",
    "In conclusion, the channels of the input tensor $\\mathbf{X}$ represent various features of the input data, and the filter $\\mathbf{W}$ is used to extract these features by convolving it with $\\mathbf{X}$. By using a filter with the same number of channels as the input, each channel's features are processed, allowing the network to learn from each aspect of the input data separately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Convolutional Layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a CNN, the input tensor is generally denoted as $\\mathbf{X} \\in \\mathbb{R}^{n_0 \\times n_0 \\times m_0}$, where  $n_0 \\times n_0$ represents the spatial dimensions of a 2D input image (e.g., pixels), and $ m_0$ represents the depth, such as 1 for grayscale images or 3 for RGB color images. This input tensor is convolved with multiple filters designed to extract relevant information or features. After convolution, these feature are passed through activation functions within the convolutional layer  $\\mathbf{Z}^1$, which introduce non-linear properties to the model. The process then repeat itself for the subsequent convolutional layers. \n",
    "\n",
    "To exemplify this process, consider two filters $\\mathbf{W}_1$ and $\\mathbf{W}_2$, which convolve with the input tensor $\\mathbf{X}$ to pass the extracted features to the the convolutional layer  $\\mathbf{Z}^1 \\in \\mathbb{R}^{n_1 \\times n_1 \\times m_1}$:\n",
    "\n",
    "\n",
    "<center><img src = \"figures/conv-input-hidden.png\" /></center>\n",
    "\n",
    "Considering a input tensor $\\mathbf{X}$ with spatial dimension $n_0 = 3$ and $m_0 = 3$ channels convolved with two filters $\\mathbf{W}_1$ and $\\mathbf{W}_2$ with window size  $k = 2$, we obtain the output tensor at layer $\\mathbf{Z}^1$. the dimension of $\\mathbf{Z}^1$ is \n",
    "$(n_0 - k + 1) \\times (n_0 - k + 1) \\times m_1$, resulting in a $2 \\times 2 \\times 2$ tensor, where $m_1 = 2$ corresponding to the number of filters applied.\n",
    "\n",
    "Let's incorporate the bias terms $b_1, b_2 \\in \\mathbb{R}$ corresponding to each filter $\\mathbf{W}_1$ and $\\mathbf{W}_2$, and a activation function $f(~ . ~)$. We represent a single  forward step in our simplified visualization of a CNN, going from input to the subsequent layer as:\n",
    "\n",
    "$$\n",
    "{\\mathbf{net}^1}  = \n",
    "\\begin{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        \\text{Sum}(X_2(1,1) \\odot \\mathbf{W}_1)+b_1& \\text{Sum}(X_2(1,2) \\odot \\mathbf{W}_1) +b_1\n",
    "        \\\\\n",
    "        \\\\\n",
    "        \\text{Sum}(X_2(2,1) \\odot \\mathbf{W}_1)+b_1& \\text{Sum}(X_2(2,2) \\odot \\mathbf{W}_1)+b_1\n",
    "    \\end{bmatrix}\\\\\n",
    "    \\\\\n",
    "    \\\\\n",
    "    \\begin{bmatrix}\n",
    "    \\text{Sum}(X_2(1,1)  \\odot \\mathbf{W}_2)+b_2& \\text{Sum}(X_2(1,2) \\odot \\mathbf{W}_2) +b_2\n",
    "    \\\\\n",
    "    \\\\\n",
    "    \\text{Sum}(X_2(2,1) \\odot \\mathbf{W}_2)+b_2& \\text{Sum}(X_2(2,2) \\odot \\mathbf{W}_2)+b_2\n",
    "    \\end{bmatrix}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The net signal after convolution is followed by the activation function to introduce non-linearity:\n",
    "\n",
    "$$\n",
    "{\\mathbf{Z}^1} = f(\\mathbf{net}^1) = \n",
    "\\begin{bmatrix}\n",
    "\\\\\n",
    "    \\begin{bmatrix}\n",
    "        f(\\text{Sum}(X_2(1,1) \\odot \\mathbf{W}_1) +b_1)& f(\\text{Sum}(X_2(1,2) \\odot \\mathbf{W}_1)+b_1) \n",
    "        \\\\\n",
    "        \\\\\n",
    "        f(\\text{Sum}(X_2(2,1) \\odot \\mathbf{W}_1)+b_1)& f(\\text{Sum}(X_2(2,2) \\odot \\mathbf{W}_1)+b_1)\n",
    "    \\end{bmatrix}\\\\\n",
    "    \\\\\n",
    "    \\\\\n",
    "    \\begin{bmatrix}\n",
    "    f(\\text{Sum}(X_2(1,1) \\odot \\mathbf{W}_2) + b_2)& f(\\text{Sum}(X_2(1,2) \\odot \\mathbf{W}_2) + b_2)\n",
    "    \\\\\n",
    "    \\\\\n",
    "    f(\\text{Sum}(X_2(2,1) \\odot \\mathbf{W}_2)+ b_2)& f(\\text{Sum}(X_2(2,2) \\odot \\mathbf{W}_2)+ b_2)\n",
    "    \\end{bmatrix}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Resulting in the feature map, which represents the actual output of the layer after applying the activation function to the net signal. The activation function can be one commonly used in neural networks, such as identity, sigmoid, tanh, or ReLU. In the language of convolutions, this is simplified as:\n",
    "\n",
    "$$\n",
    "{\\mathbf{Z}^1} = \n",
    "\\begin{bmatrix}\n",
    "    f(\\mathbf{X} * \\mathbf{W}_1 \\oplus b_1)\\\\\n",
    "    \\\\\n",
    "    f(\\mathbf{X} * \\mathbf{W}_2 \\oplus b_2)\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "where $\\oplus$ denotes the addition of the bias term to each element of the feature maps produced by $\\mathbf{X} * \\mathbf{W}_1$ and $\\mathbf{X} * \\mathbf{W}_2$. For a compact representation:\n",
    "\n",
    "$$\n",
    "{\\mathbf{Z}^1} = f(\\mathbf{X} * \\mathbf{W}_1 \\oplus b_1,  \\mathbf{X} * \\mathbf{W}_2 \\oplus b_2)\n",
    "$$\n",
    "\n",
    "Extending this concept to a more general and formal case, from the $l$-th layer to the $l+1$-th layer with multiple filters, we denote the tensor at layer $l$ as $\\mathbf{Z}^l \\in \\mathbb{R}^{n_l \\times n_l \\times m_l}$. Each element $Z_{i,j,q}^l$ of the tensor represents the output value of a neuron located at row $i$, column $j$, and channel $q$ for layer $l$, where $1 \\leq (i,j) \\leq n_l$ and $1 \\leq q \\leq m_l$. Assuming we have $m_{l+1}$ filters $\\{\\mathbf{W}_1, \\cdots, \\mathbf{W}_{m_{l+1}}\\}$, the output feature map passed through the next layer will have $m_{l+1}$ channels.\n",
    "\n",
    "The convolution $f(\\mathbf{Z}^l * \\mathbf{W}_q \\oplus b_q)$ for a given filter $q$ produces a feature map matrix of dimension $(n_l-k+1) \\times (n_l-k+1)$, where each element of this feature map corresponds to a neuron's output at layer $l+1$. Convolving $\\mathbf{Z}^l$ with all $m_{l+1}$ filters, we form the tensor $\\mathbf{Z}^{l+1}$ for layer $l+1$ with dimensions $(n_l-k+1) \\times (n_l-k+1) \\times m_{l+1}$. The result for this tensor at layer $l+1$ is:\n",
    "\n",
    "$$\n",
    "{\\mathbf{Z}^{l+1}} = f\\bigg(\\big(\\mathbf{Z}^l * \\mathbf{W}_1 \\oplus b_1\\big), \\cdots,\\big(\\mathbf{Z}^l * \\mathbf{W}_q \\oplus b_q\\big), \\cdots, \\big( \\mathbf{Z}^l * \\mathbf{W}_{m_{l+1}} \\oplus b_{m_{l+1}}\\big)\\bigg)\n",
    "$$\n",
    "\n",
    "In summary, a Convolutional Layer takes as input the $n_l \\times n_l \\times m_l$ tensor  $\\mathbf{Z}^l$ of neurons from layer $l$, and then computes the $n_{l+1} \\times n_{l+1} \\times m_{l+1}$ tensor $\\mathbf{Z}^{l+1}$ of neurons for the next layer $l+1$ via the convolution of $\\mathbf{Z}^{l}$ with $m_{l+1}$ different  filters of size $k \\times k \\times m_l$, followd by adding the bias and applying some non-linear activation function $f(~.~)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Padding and Striding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the problem with the convolution operation is that the size of the tensor will decrease in each successive layer. If a tensor $\\mathbf{Z}^l$ at layer $l$ has size $n_l \\times n_l \\times m_l$, and  we use filters of size $k \\times k \\times m_l$, then each channel in a layer $l+1$ will have size $(n_l - (k - 1)) \\times (n_l-(k -1))$. That is, the number of rows and columns for each successive tensor will shrink by $k-1$.\n",
    "\n",
    "\n",
    "**Padding**\n",
    "\n",
    "Padding involves adding zeros or other values around the edges of the input data before applying a convolutional filter. The purpose of padding is to preserve the spatial dimensions of the input data in the output feature map. Without padding, the spatial dimensions of the output feature map would be reduced after each convolutional layer, leading to the loss of important spatial information. By adding padding, the spatial dimensions of the output feature map can be preserved or even increased.\n",
    "\n",
    "Assume that we add $p$ rows and columns of zeros. With padding $p$, the new dimension of tensor $\\mathbf{Z}^l$ at layer $l$ is $(n_l + 2p) \\times (n_l +2p) \\times m_l$. Assuming that each filter is of size $k \\times k \\times m_l$, and that there are $m_{l+1}$ filters, then the size of tensor $\\mathbf{Z}^{l+1}$ at layer $l+1$ will be $(n_l + 2p -(k-1)) \\times (n_l + 2p-(k-1)) \\times m_{l+1}$. Since we want to preserve or increase the size of the resulting tensor, we need to have the following lower bound when choosing the padding:\n",
    "\n",
    "$$n_l +2p - k + 1  \\geq n_l$$\n",
    "\n",
    "which implies $p \\geq \\frac{k-1}{2}$. So the result size after convolution with padding $p$ will be \n",
    "\n",
    "$$Dim(\\mathbf{Z}^{l+1})= (n_l + 2p -(k-1)) \\times (n_l + 2p-(k-1)) \\times m_{l+1}.$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Striding**\n",
    "\n",
    "Striding, on the other hand, involves controls the slide size steps of the filter channels across the sub tensor (or window) of  $\\mathbf{Z}^l$ in the convolution operation. Until now we implicitly use a stride of size $s = 1$ as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Sum}(\\mathbf{Z}^l_k(i,j) \\odot \\mathbf{W})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "for $(i,j) = 1,2, \\cdots, n_l-k+1$, where the indices $i$ and $j$ increase by $s = 1$ at each step. For a given stride $s$, the set of indices $(i,j)$ can be written as:\n",
    "\n",
    "$$\n",
    "\\text{for stride } s, \\quad (i,j) = 1 + 0\\cdot s,1+1 \\cdot s,1+2\\cdot s, \\cdots,1 + t\\cdot s\n",
    "$$\n",
    "\n",
    "where $t$ is the largest integer such that $1 + ts \\leq n_l - k + 1$. This ensures that the applied filter starting from the first element and slide it over the matrix by $s$ elements each time, stopping at the correct boundary without exceeding the size of the window matrix. this results in \n",
    "\n",
    "$$\n",
    "t \\leq \\left\\lfloor \\frac{n_l - k}{s} \\right\\rfloor\n",
    "$$\n",
    "\n",
    "the symbol $\\lfloor ~ \\rfloor$ means rounding down to the nearest whole number (since we cannot have a fraction of a step).\n",
    "\n",
    "Taking the convolution of $\\mathbf{Z}^l$ with size $n_l \\times n_l \\times m_l$ with a filter $\\mathbf{W}$ of size $k \\times k \\times m_l$ and stride $s \\geq 1$ would give:\n",
    "\n",
    "$$\\mathbf{Z}^l\\ast \\mathbf{W} = \\begin{bmatrix}\n",
    "\\text{Sum}(\\mathbf{Z}^l_k(1,1) \\odot \\mathbf{W}) &\\text{Sum}(\\mathbf{Z}^l_k(1,1+s) \\odot \\mathbf{W}) & \\cdots & \\text{Sum}(\\mathbf{Z}^l_k(1,1+t.s) \\odot \\mathbf{W}) \\\\\n",
    "\\text{Sum}(\\mathbf{Z}^l_k(1+s,1) \\odot \\mathbf{W}) & \\text{Sum}(\\mathbf{Z}^l_k(1+s,1+s) \\odot \\mathbf{W}) & \\cdots &\\text{Sum}(\\mathbf{Z}^l_k(1+s,1+t.s) \\odot \\mathbf{W})\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\text{Sum}(\\mathbf{Z}^l_k(1+t.s,1) \\odot \\mathbf{W}) & \\text{Sum}(\\mathbf{Z}^l_k(1+t.s,1+s) \\odot \\mathbf{W}) & \\cdots & \\text{Sum}(\\mathbf{Z}^l_k(1+t.s,1+t.s) \\odot \\mathbf{W})\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "where $t \\leq \\left\\lfloor \\frac{n_l - k}{s} \\right\\rfloor$. So, the result dimension after convolution with striding $s$ for a set of $m_{l+1}$ filters will be\n",
    "\n",
    "$$Dim(\\mathbf{Z}^{l+1})= \\bigg(\\left\\lfloor \\frac{n_l - k}{s} \\right\\rfloor + 1\\bigg) \\times \\bigg(\\left\\lfloor \\frac{n_l - k}{s} \\right\\rfloor + 1\\bigg) \\times m_{l+1}.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Max-pooling Layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we replace the summation with the maximum value over the element-wise product of $\\mathbf{{Z}_k^l}$ and $\\mathbf{W}$, we get\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Sum}(\\mathbf{Z}_k(i,j) \\odot \\mathbf{W}) &\\longrightarrow \\text{Max}(\\mathbf{Z}_k(i,j))\\\\\n",
    "\\\\\n",
    "\\sum_{a=1}^{k}\\sum_{b=1}^{k}\\sum_{c=1}^{m_l}z^l_{(i+a-1),(j+b-1),~c}~w_{a,b,c} &~\\longrightarrow \n",
    "\\max_{\\substack{a ~= 1,\\cdots,k \\\\ b~ = 1,\\cdots,k \\\\ c = 1,\\cdots,m_l}}{\\bigg\\{z^l_{(i+a-1),(j+b-1),~c}~\\cdot w_{a,b,c} \\bigg\\}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The convolution of $\\mathbf{Z}^l \\in \\mathbb{R}^{n_l \\times n_l \\times m_l}$ with filter $\\mathbf{W} \\in \\mathbb{R}^{k \\times k \\times 1}$ using max-pooling, denoted $\\mathbf{Z}^l \\ast_{max} ~ \\mathbf{W}$, restuls in a $(n_l -k +1) \\times (n_l -k+1) \\times 1$ (for a single filter) is given by:\n",
    "\n",
    "$$\n",
    "\\mathbf{Z}_k^l \\ast_{max} \\mathbf{W} = \n",
    "\\begin{bmatrix}\n",
    "\\text{Max}(Z^l_k(1,1) \\odot \\mathbf{W})  & \\cdots & \\text{Max}(Z^l_k(1,n-k+1) \\odot \\mathbf{W}) \\\\\n",
    "\\vdots  & \\ddots & \\vdots \\\\\n",
    "\\text{Max}(Z^l_k(n-k+1,1) \\odot \\mathbf{W}) & \\cdots & \\text{Max}(Z^l_k(n-k+1,n-k+1) \\odot \\mathbf{W})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Typically, max-pooling its common to set the stride equal to the filter size ($s = k$), so that the aggregation function is applied over disjoint $k \\times k$ windows in each channel in $\\mathbf{Z}^l$. In pooling layer, a filter $\\mathbf{W}$ is by default a $k \\times k \\times 1$ tensor of fixed value of ones, so that $\\mathbf{W} = \\mathbf{1}_{k \\times k \\times 1} $. This means that the filters will not be updated during the backpropagation. Also, the filters has no bias term, they all are fixed as zeros. The convolution of $\\mathbf{Z}^l \\in \\mathbb{R}^{n_l \\times n_l \\times m_l}$ with  $\\mathbf{W} \\in \\mathbb{R}^{k \\times k \\times 1}$ , using stride $s = k$, results in  a tensor $\\mathbf{Z}^{l+1}$ of size $\\left\\lfloor \\frac{n_l}{k} \\right\\rfloor \\times \\left\\lfloor \\frac{n_l}{k} \\right\\rfloor \\times m_l$.\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{Z}^{l+1} = \\mathbf{Z}_k^l \\ast_{max} \\mathbf{W} = \n",
    "\\begin{bmatrix}\n",
    "\\text{Max}(Z^l_k(1,1) \\odot \\mathbf{W})  & \\cdots & \\text{Max}(Z^l_k(1,\\left\\lfloor \\frac{n_l}{k} \\right\\rfloor ) \\odot \\mathbf{W}) \\\\\n",
    "\\vdots  & \\ddots & \\vdots \\\\\n",
    "\\text{Max}(Z^l_k(\\left\\lfloor \\frac{n_l}{k} \\right\\rfloor,1) \\odot \\mathbf{W}) & \\cdots & \\text{Max}(Z^l_k(\\left\\lfloor \\frac{n_l}{k} \\right\\rfloor,\\left\\lfloor \\frac{n_l}{k} \\right\\rfloor) \\odot \\mathbf{W})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Note that the max pooling layer do not uses any activation function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training CNNs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Homework**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \n",
    "    train = pd.read_csv('data/mnist_train_small.csv')\n",
    "    test = pd.read_csv('data/mnist_test_small.csv')\n",
    "\n",
    "    y_train = train['label'].to_numpy()\n",
    "    x_train = train.drop('label', axis = 1).to_numpy()\n",
    "    y_test = test['label'].to_numpy()\n",
    "    x_test = test.drop('label', axis = 1).to_numpy()\n",
    "    \n",
    "    # reshape dataset to have a single channel\n",
    "    # Reshape (20000,784) ->  (2000,     28,     28,     1)\n",
    "    x_test = x_test.reshape(x_test.shape[0],28,28,1)\n",
    "    x_train = x_train.reshape(x_train.shape[0],28,28,1)\n",
    "    # one hot encode target values\n",
    "    y_test = to_categorical(y_test)\n",
    "    y_train = to_categorical(y_train)\n",
    "\n",
    "    return (x_train, y_train), (x_train, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
